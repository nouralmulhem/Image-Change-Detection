{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Kaiyu Li\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# https://github.com/likyoo\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mconv_block_nested\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Kaiyu Li\n",
    "# https://github.com/likyoo\n",
    "#\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class conv_block_nested(nn.Module):\n",
    "    def __init__(self, in_ch, mid_ch, out_ch):\n",
    "        super(conv_block_nested, self).__init__()\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(mid_ch)\n",
    "        self.conv2 = nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        identity = x\n",
    "        x = self.bn1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        output = self.activation(x + identity)\n",
    "        return output\n",
    "\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, bilinear=False):\n",
    "        super(up, self).__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2,\n",
    "                                  mode='bilinear',\n",
    "                                  align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch, in_ch, 2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, ratio = 16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_channels,in_channels//ratio,1,bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(in_channels//ratio, in_channels,1,bias=False)\n",
    "        self.sigmod = nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmod(out)\n",
    "\n",
    "\n",
    "\n",
    "class SNUNet_ECAM(nn.Module):\n",
    "    # SNUNet-CD with ECAM\n",
    "    def __init__(self, in_ch=3, out_ch=2):\n",
    "        super(SNUNet_ECAM, self).__init__()\n",
    "        torch.nn.Module.dump_patches = True\n",
    "        n1 = 32     # the initial number of channels of feature map\n",
    "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv0_0 = conv_block_nested(in_ch, filters[0], filters[0])\n",
    "        self.conv1_0 = conv_block_nested(filters[0], filters[1], filters[1])\n",
    "        self.Up1_0 = up(filters[1])\n",
    "        self.conv2_0 = conv_block_nested(filters[1], filters[2], filters[2])\n",
    "        self.Up2_0 = up(filters[2])\n",
    "        self.conv3_0 = conv_block_nested(filters[2], filters[3], filters[3])\n",
    "        self.Up3_0 = up(filters[3])\n",
    "        self.conv4_0 = conv_block_nested(filters[3], filters[4], filters[4])\n",
    "        self.Up4_0 = up(filters[4])\n",
    "\n",
    "        self.conv0_1 = conv_block_nested(filters[0] * 2 + filters[1], filters[0], filters[0])\n",
    "        self.conv1_1 = conv_block_nested(filters[1] * 2 + filters[2], filters[1], filters[1])\n",
    "        self.Up1_1 = up(filters[1])\n",
    "        self.conv2_1 = conv_block_nested(filters[2] * 2 + filters[3], filters[2], filters[2])\n",
    "        self.Up2_1 = up(filters[2])\n",
    "        self.conv3_1 = conv_block_nested(filters[3] * 2 + filters[4], filters[3], filters[3])\n",
    "        self.Up3_1 = up(filters[3])\n",
    "\n",
    "        self.conv0_2 = conv_block_nested(filters[0] * 3 + filters[1], filters[0], filters[0])\n",
    "        self.conv1_2 = conv_block_nested(filters[1] * 3 + filters[2], filters[1], filters[1])\n",
    "        self.Up1_2 = up(filters[1])\n",
    "        self.conv2_2 = conv_block_nested(filters[2] * 3 + filters[3], filters[2], filters[2])\n",
    "        self.Up2_2 = up(filters[2])\n",
    "\n",
    "        self.conv0_3 = conv_block_nested(filters[0] * 4 + filters[1], filters[0], filters[0])\n",
    "        self.conv1_3 = conv_block_nested(filters[1] * 4 + filters[2], filters[1], filters[1])\n",
    "        self.Up1_3 = up(filters[1])\n",
    "\n",
    "        self.conv0_4 = conv_block_nested(filters[0] * 5 + filters[1], filters[0], filters[0])\n",
    "\n",
    "        self.ca = ChannelAttention(filters[0] * 4, ratio=16)\n",
    "        self.ca1 = ChannelAttention(filters[0], ratio=16 // 4)\n",
    "\n",
    "        self.conv_final = nn.Conv2d(filters[0] * 4, out_ch, kernel_size=1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "    def forward(self, xA, xB):\n",
    "        '''xA'''\n",
    "        x0_0A = self.conv0_0(xA)\n",
    "        x1_0A = self.conv1_0(self.pool(x0_0A))\n",
    "        x2_0A = self.conv2_0(self.pool(x1_0A))\n",
    "        x3_0A = self.conv3_0(self.pool(x2_0A))\n",
    "        # x4_0A = self.conv4_0(self.pool(x3_0A))\n",
    "        '''xB'''\n",
    "        x0_0B = self.conv0_0(xB)\n",
    "        x1_0B = self.conv1_0(self.pool(x0_0B))\n",
    "        x2_0B = self.conv2_0(self.pool(x1_0B))\n",
    "        x3_0B = self.conv3_0(self.pool(x2_0B))\n",
    "        x4_0B = self.conv4_0(self.pool(x3_0B))\n",
    "\n",
    "        x0_1 = self.conv0_1(torch.cat([x0_0A, x0_0B, self.Up1_0(x1_0B)], 1))\n",
    "        x1_1 = self.conv1_1(torch.cat([x1_0A, x1_0B, self.Up2_0(x2_0B)], 1))\n",
    "        x0_2 = self.conv0_2(torch.cat([x0_0A, x0_0B, x0_1, self.Up1_1(x1_1)], 1))\n",
    "\n",
    "\n",
    "        x2_1 = self.conv2_1(torch.cat([x2_0A, x2_0B, self.Up3_0(x3_0B)], 1))\n",
    "        x1_2 = self.conv1_2(torch.cat([x1_0A, x1_0B, x1_1, self.Up2_1(x2_1)], 1))\n",
    "        x0_3 = self.conv0_3(torch.cat([x0_0A, x0_0B, x0_1, x0_2, self.Up1_2(x1_2)], 1))\n",
    "\n",
    "        x3_1 = self.conv3_1(torch.cat([x3_0A, x3_0B, self.Up4_0(x4_0B)], 1))\n",
    "        x2_2 = self.conv2_2(torch.cat([x2_0A, x2_0B, x2_1, self.Up3_1(x3_1)], 1))\n",
    "        x1_3 = self.conv1_3(torch.cat([x1_0A, x1_0B, x1_1, x1_2, self.Up2_2(x2_2)], 1))\n",
    "        x0_4 = self.conv0_4(torch.cat([x0_0A, x0_0B, x0_1, x0_2, x0_3, self.Up1_3(x1_3)], 1))\n",
    "\n",
    "        out = torch.cat([x0_1, x0_2, x0_3, x0_4], 1)\n",
    "\n",
    "        intra = torch.sum(torch.stack((x0_1, x0_2, x0_3, x0_4)), dim=0)\n",
    "        ca1 = self.ca1(intra)\n",
    "        out = self.ca(out) * (out + ca1.repeat(1, 4, 1, 1))\n",
    "        out = self.conv_final(out)\n",
    "\n",
    "        return (out, )\n",
    "\n",
    "\n",
    "class Siam_NestedUNet_Conc(nn.Module):\n",
    "    # SNUNet-CD without Attention\n",
    "    def __init__(self, in_ch=3, out_ch=2):\n",
    "        super(Siam_NestedUNet_Conc, self).__init__()\n",
    "        torch.nn.Module.dump_patches = True\n",
    "        n1 = 32     # the initial number of channels of feature map\n",
    "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv0_0 = conv_block_nested(in_ch, filters[0], filters[0])\n",
    "        self.conv1_0 = conv_block_nested(filters[0], filters[1], filters[1])\n",
    "        self.Up1_0 = up(filters[1])\n",
    "        self.conv2_0 = conv_block_nested(filters[1], filters[2], filters[2])\n",
    "        self.Up2_0 = up(filters[2])\n",
    "        self.conv3_0 = conv_block_nested(filters[2], filters[3], filters[3])\n",
    "        self.Up3_0 = up(filters[3])\n",
    "        self.conv4_0 = conv_block_nested(filters[3], filters[4], filters[4])\n",
    "        self.Up4_0 = up(filters[4])\n",
    "\n",
    "        self.conv0_1 = conv_block_nested(filters[0] * 2 + filters[1], filters[0], filters[0])\n",
    "        self.conv1_1 = conv_block_nested(filters[1] * 2 + filters[2], filters[1], filters[1])\n",
    "        self.Up1_1 = up(filters[1])\n",
    "        self.conv2_1 = conv_block_nested(filters[2] * 2 + filters[3], filters[2], filters[2])\n",
    "        self.Up2_1 = up(filters[2])\n",
    "        self.conv3_1 = conv_block_nested(filters[3] * 2 + filters[4], filters[3], filters[3])\n",
    "        self.Up3_1 = up(filters[3])\n",
    "\n",
    "        self.conv0_2 = conv_block_nested(filters[0] * 3 + filters[1], filters[0], filters[0])\n",
    "        self.conv1_2 = conv_block_nested(filters[1] * 3 + filters[2], filters[1], filters[1])\n",
    "        self.Up1_2 = up(filters[1])\n",
    "        self.conv2_2 = conv_block_nested(filters[2] * 3 + filters[3], filters[2], filters[2])\n",
    "        self.Up2_2 = up(filters[2])\n",
    "\n",
    "        self.conv0_3 = conv_block_nested(filters[0] * 4 + filters[1], filters[0], filters[0])\n",
    "        self.conv1_3 = conv_block_nested(filters[1] * 4 + filters[2], filters[1], filters[1])\n",
    "        self.Up1_3 = up(filters[1])\n",
    "\n",
    "        self.conv0_4 = conv_block_nested(filters[0] * 5 + filters[1], filters[0], filters[0])\n",
    "\n",
    "        self.final1 = nn.Conv2d(filters[0], out_ch, kernel_size=1)\n",
    "        self.final2 = nn.Conv2d(filters[0], out_ch, kernel_size=1)\n",
    "        self.final3 = nn.Conv2d(filters[0], out_ch, kernel_size=1)\n",
    "        self.final4 = nn.Conv2d(filters[0], out_ch, kernel_size=1)\n",
    "        self.conv_final = nn.Conv2d(out_ch * 4, out_ch, kernel_size=1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "    def forward(self, xA, xB):\n",
    "        '''xA'''\n",
    "        x0_0A = self.conv0_0(xA)\n",
    "        x1_0A = self.conv1_0(self.pool(x0_0A))\n",
    "        x2_0A = self.conv2_0(self.pool(x1_0A))\n",
    "        x3_0A = self.conv3_0(self.pool(x2_0A))\n",
    "        # x4_0A = self.conv4_0(self.pool(x3_0A))\n",
    "        '''xB'''\n",
    "        x0_0B = self.conv0_0(xB)\n",
    "        x1_0B = self.conv1_0(self.pool(x0_0B))\n",
    "        x2_0B = self.conv2_0(self.pool(x1_0B))\n",
    "        x3_0B = self.conv3_0(self.pool(x2_0B))\n",
    "        x4_0B = self.conv4_0(self.pool(x3_0B))\n",
    "\n",
    "        x0_1 = self.conv0_1(torch.cat([x0_0A, x0_0B, self.Up1_0(x1_0B)], 1))\n",
    "        x1_1 = self.conv1_1(torch.cat([x1_0A, x1_0B, self.Up2_0(x2_0B)], 1))\n",
    "        x0_2 = self.conv0_2(torch.cat([x0_0A, x0_0B, x0_1, self.Up1_1(x1_1)], 1))\n",
    "\n",
    "\n",
    "        x2_1 = self.conv2_1(torch.cat([x2_0A, x2_0B, self.Up3_0(x3_0B)], 1))\n",
    "        x1_2 = self.conv1_2(torch.cat([x1_0A, x1_0B, x1_1, self.Up2_1(x2_1)], 1))\n",
    "        x0_3 = self.conv0_3(torch.cat([x0_0A, x0_0B, x0_1, x0_2, self.Up1_2(x1_2)], 1))\n",
    "\n",
    "        x3_1 = self.conv3_1(torch.cat([x3_0A, x3_0B, self.Up4_0(x4_0B)], 1))\n",
    "        x2_2 = self.conv2_2(torch.cat([x2_0A, x2_0B, x2_1, self.Up3_1(x3_1)], 1))\n",
    "        x1_3 = self.conv1_3(torch.cat([x1_0A, x1_0B, x1_1, x1_2, self.Up2_2(x2_2)], 1))\n",
    "        x0_4 = self.conv0_4(torch.cat([x0_0A, x0_0B, x0_1, x0_2, x0_3, self.Up1_3(x1_3)], 1))\n",
    "\n",
    "\n",
    "        output1 = self.final1(x0_1)\n",
    "        output2 = self.final2(x0_2)\n",
    "        output3 = self.final3(x0_3)\n",
    "        output4 = self.final4(x0_4)\n",
    "        output = self.conv_final(torch.cat([output1, output2, output3, output4], 1))\n",
    "        return (output1, output2, output3, output4, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rodrigo Caye Daudt\n",
    "# https://rcdaudt.github.io/\n",
    "# Daudt, R. C., Le Saux, B., & Boulch, A. \"Fully convolutional siamese networks for change detection\". In 2018 25th IEEE International Conference on Image Processing (ICIP) (pp. 4063-4067). IEEE.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.padding import ReplicationPad2d\n",
    "\n",
    "class SiamUnet_diff(nn.Module):\n",
    "    \"\"\"SiamUnet_diff segmentation network.\"\"\"\n",
    "\n",
    "    def __init__(self, input_nbr, label_nbr):\n",
    "        super(SiamUnet_diff, self).__init__()\n",
    "\n",
    "        n1 = 16\n",
    "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
    "        self.input_nbr = input_nbr\n",
    "\n",
    "        self.conv11 = nn.Conv2d(input_nbr, filters[0], kernel_size=3, padding=1)\n",
    "        self.bn11 = nn.BatchNorm2d(filters[0])\n",
    "        self.do11 = nn.Dropout2d(p=0.2)\n",
    "        self.conv12 = nn.Conv2d(filters[0], filters[0], kernel_size=3, padding=1)\n",
    "        self.bn12 = nn.BatchNorm2d(filters[0])\n",
    "        self.do12 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.conv21 = nn.Conv2d(filters[0], filters[1], kernel_size=3, padding=1)\n",
    "        self.bn21 = nn.BatchNorm2d(filters[1])\n",
    "        self.do21 = nn.Dropout2d(p=0.2)\n",
    "        self.conv22 = nn.Conv2d(filters[1], filters[1], kernel_size=3, padding=1)\n",
    "        self.bn22 = nn.BatchNorm2d(filters[1])\n",
    "        self.do22 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.conv31 = nn.Conv2d(filters[1], filters[2], kernel_size=3, padding=1)\n",
    "        self.bn31 = nn.BatchNorm2d(filters[2])\n",
    "        self.do31 = nn.Dropout2d(p=0.2)\n",
    "        self.conv32 = nn.Conv2d(filters[2], filters[2], kernel_size=3, padding=1)\n",
    "        self.bn32 = nn.BatchNorm2d(filters[2])\n",
    "        self.do32 = nn.Dropout2d(p=0.2)\n",
    "        self.conv33 = nn.Conv2d(filters[2], filters[2], kernel_size=3, padding=1)\n",
    "        self.bn33 = nn.BatchNorm2d(filters[2])\n",
    "        self.do33 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.conv41 = nn.Conv2d(filters[2], filters[3], kernel_size=3, padding=1)\n",
    "        self.bn41 = nn.BatchNorm2d(filters[3])\n",
    "        self.do41 = nn.Dropout2d(p=0.2)\n",
    "        self.conv42 = nn.Conv2d(filters[3], filters[3], kernel_size=3, padding=1)\n",
    "        self.bn42 = nn.BatchNorm2d(filters[3])\n",
    "        self.do42 = nn.Dropout2d(p=0.2)\n",
    "        self.conv43 = nn.Conv2d(filters[3], filters[3], kernel_size=3, padding=1)\n",
    "        self.bn43 = nn.BatchNorm2d(filters[3])\n",
    "        self.do43 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(filters[3], filters[3], kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv43d = nn.ConvTranspose2d(filters[4], filters[3], kernel_size=3, padding=1)\n",
    "        self.bn43d = nn.BatchNorm2d(filters[3])\n",
    "        self.do43d = nn.Dropout2d(p=0.2)\n",
    "        self.conv42d = nn.ConvTranspose2d(filters[3], filters[3], kernel_size=3, padding=1)\n",
    "        self.bn42d = nn.BatchNorm2d(filters[3])\n",
    "        self.do42d = nn.Dropout2d(p=0.2)\n",
    "        self.conv41d = nn.ConvTranspose2d(filters[3], filters[2], kernel_size=3, padding=1)\n",
    "        self.bn41d = nn.BatchNorm2d(filters[2])\n",
    "        self.do41d = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(filters[2], filters[2], kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv33d = nn.ConvTranspose2d(filters[3], filters[2], kernel_size=3, padding=1)\n",
    "        self.bn33d = nn.BatchNorm2d(filters[2])\n",
    "        self.do33d = nn.Dropout2d(p=0.2)\n",
    "        self.conv32d = nn.ConvTranspose2d(filters[2], filters[2], kernel_size=3, padding=1)\n",
    "        self.bn32d = nn.BatchNorm2d(filters[2])\n",
    "        self.do32d = nn.Dropout2d(p=0.2)\n",
    "        self.conv31d = nn.ConvTranspose2d(filters[2], filters[1], kernel_size=3, padding=1)\n",
    "        self.bn31d = nn.BatchNorm2d(filters[1])\n",
    "        self.do31d = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(filters[1], filters[1], kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv22d = nn.ConvTranspose2d(filters[2], filters[1], kernel_size=3, padding=1)\n",
    "        self.bn22d = nn.BatchNorm2d(filters[1])\n",
    "        self.do22d = nn.Dropout2d(p=0.2)\n",
    "        self.conv21d = nn.ConvTranspose2d(filters[1], filters[0], kernel_size=3, padding=1)\n",
    "        self.bn21d = nn.BatchNorm2d(filters[0])\n",
    "        self.do21d = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(filters[0], filters[0], kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv12d = nn.ConvTranspose2d(filters[1], filters[0], kernel_size=3, padding=1)\n",
    "        self.bn12d = nn.BatchNorm2d(filters[0])\n",
    "        self.do12d = nn.Dropout2d(p=0.2)\n",
    "        self.conv11d = nn.ConvTranspose2d(filters[0], label_nbr, kernel_size=3, padding=1)\n",
    "\n",
    "        self.sm = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "\n",
    "        \"\"\"Forward method.\"\"\"\n",
    "        # Stage 1\n",
    "        x11 = self.do11(F.relu(self.bn11(self.conv11(x1))))\n",
    "        x12_1 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
    "        x1p = F.max_pool2d(x12_1, kernel_size=2, stride=2)\n",
    "\n",
    "\n",
    "        # Stage 2\n",
    "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
    "        x22_1 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
    "        x2p = F.max_pool2d(x22_1, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 3\n",
    "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
    "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
    "        x33_1 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
    "        x3p = F.max_pool2d(x33_1, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 4\n",
    "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
    "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
    "        x43_1 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
    "        x4p = F.max_pool2d(x43_1, kernel_size=2, stride=2)\n",
    "\n",
    "        ####################################################\n",
    "        # Stage 1\n",
    "        x11 = self.do11(F.relu(self.bn11(self.conv11(x2))))\n",
    "        x12_2 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
    "        x1p = F.max_pool2d(x12_2, kernel_size=2, stride=2)\n",
    "\n",
    "\n",
    "        # Stage 2\n",
    "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
    "        x22_2 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
    "        x2p = F.max_pool2d(x22_2, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 3\n",
    "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
    "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
    "        x33_2 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
    "        x3p = F.max_pool2d(x33_2, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 4\n",
    "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
    "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
    "        x43_2 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
    "        x4p = F.max_pool2d(x43_2, kernel_size=2, stride=2)\n",
    "\n",
    "\n",
    "\n",
    "        # Stage 4d\n",
    "        x4d = self.upconv4(x4p)\n",
    "        pad4 = ReplicationPad2d((0, x43_1.size(3) - x4d.size(3), 0, x43_1.size(2) - x4d.size(2)))\n",
    "        x4d = torch.cat((pad4(x4d), torch.abs(x43_1 - x43_2)), 1)\n",
    "        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n",
    "        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n",
    "        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n",
    "\n",
    "        # Stage 3d\n",
    "        x3d = self.upconv3(x41d)\n",
    "        pad3 = ReplicationPad2d((0, x33_1.size(3) - x3d.size(3), 0, x33_1.size(2) - x3d.size(2)))\n",
    "        x3d = torch.cat((pad3(x3d), torch.abs(x33_1 - x33_2)), 1)\n",
    "        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n",
    "        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n",
    "        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n",
    "\n",
    "        # Stage 2d\n",
    "        x2d = self.upconv2(x31d)\n",
    "        pad2 = ReplicationPad2d((0, x22_1.size(3) - x2d.size(3), 0, x22_1.size(2) - x2d.size(2)))\n",
    "        x2d = torch.cat((pad2(x2d), torch.abs(x22_1 - x22_2)), 1)\n",
    "        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n",
    "        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n",
    "\n",
    "        # Stage 1d\n",
    "        x1d = self.upconv1(x21d)\n",
    "        pad1 = ReplicationPad2d((0, x12_1.size(3) - x1d.size(3), 0, x12_1.size(2) - x1d.size(2)))\n",
    "        x1d = torch.cat((pad1(x1d), torch.abs(x12_1 - x12_2)), 1)\n",
    "        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n",
    "        x11d = self.conv11d(x12d)\n",
    "\n",
    "        return (x11d, )\n",
    "        # return self.sm(x11d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support as prfs\n",
    "from utils.parser import get_parser_with_args\n",
    "from utils.helpers import (get_loaders, get_criterion,\n",
    "                           load_model, initialize_metrics, get_mean_metrics,\n",
    "                           set_metrics)\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Initialize Parser and define arguments\n",
    "\"\"\"\n",
    "parser, metadata = get_parser_with_args()\n",
    "opt = parser.parse_args()\n",
    "\n",
    "\"\"\"\n",
    "Initialize experiments log\n",
    "\"\"\"\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "writer = SummaryWriter(opt.log_dir + f'/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}/')\n",
    "\n",
    "\"\"\"\n",
    "Set up environment: define paths, download data, and set device\n",
    "\"\"\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "dev = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "logging.info('GPU AVAILABLE? ' + str(torch.cuda.is_available()))\n",
    "\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(seed=777)\n",
    "\n",
    "\n",
    "train_loader, val_loader = get_loaders(opt)\n",
    "\n",
    "\"\"\"\n",
    "Load Model then define other aspects of the model\n",
    "\"\"\"\n",
    "logging.info('LOADING Model')\n",
    "model = load_model(opt, dev)\n",
    "\n",
    "criterion = get_criterion(opt)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=opt.learning_rate) # Be careful when you adjust learning rate, you can refer to the linear scaling rule\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.5)\n",
    "\n",
    "\"\"\"\n",
    " Set starting values\n",
    "\"\"\"\n",
    "best_metrics = {'cd_f1scores': -1, 'cd_recalls': -1, 'cd_precisions': -1}\n",
    "logging.info('STARTING training')\n",
    "total_step = -1\n",
    "\n",
    "for epoch in range(opt.epochs):\n",
    "    train_metrics = initialize_metrics()\n",
    "    val_metrics = initialize_metrics()\n",
    "\n",
    "    \"\"\"\n",
    "    Begin Training\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    logging.info('SET model mode to train!')\n",
    "    batch_iter = 0\n",
    "    tbar = tqdm(train_loader)\n",
    "    for batch_img1, batch_img2, labels in tbar:\n",
    "        tbar.set_description(\"epoch {} info \".format(epoch) + str(batch_iter) + \" - \" + str(batch_iter+opt.batch_size))\n",
    "        batch_iter = batch_iter+opt.batch_size\n",
    "        total_step += 1\n",
    "        # Set variables for training\n",
    "        batch_img1 = batch_img1.float().to(dev)\n",
    "        batch_img2 = batch_img2.float().to(dev)\n",
    "        labels = labels.long().to(dev)\n",
    "\n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get model predictions, calculate loss, backprop\n",
    "        cd_preds = model(batch_img1, batch_img2)\n",
    "\n",
    "        cd_loss = criterion(cd_preds, labels)\n",
    "        loss = cd_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cd_preds = cd_preds[-1]\n",
    "        _, cd_preds = torch.max(cd_preds, 1)\n",
    "\n",
    "        # Calculate and log other batch metrics\n",
    "        cd_corrects = (100 *\n",
    "                       (cd_preds.squeeze().byte() == labels.squeeze().byte()).sum() /\n",
    "                       (labels.size()[0] * (opt.patch_size**2)))\n",
    "\n",
    "        cd_train_report = prfs(labels.data.cpu().numpy().flatten(),\n",
    "                               cd_preds.data.cpu().numpy().flatten(),\n",
    "                               average='binary',\n",
    "                               zero_division=0,\n",
    "                               pos_label=1)\n",
    "\n",
    "        train_metrics = set_metrics(train_metrics,\n",
    "                                    cd_loss,\n",
    "                                    cd_corrects,\n",
    "                                    cd_train_report,\n",
    "                                    scheduler.get_last_lr())\n",
    "\n",
    "        # log the batch mean metrics\n",
    "        mean_train_metrics = get_mean_metrics(train_metrics)\n",
    "\n",
    "        for k, v in mean_train_metrics.items():\n",
    "            writer.add_scalars(str(k), {'train': v}, total_step)\n",
    "\n",
    "        # clear batch variables from memory\n",
    "        del batch_img1, batch_img2, labels\n",
    "\n",
    "    scheduler.step()\n",
    "    logging.info(\"EPOCH {} TRAIN METRICS\".format(epoch) + str(mean_train_metrics))\n",
    "\n",
    "    \"\"\"\n",
    "    Begin Validation\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_img1, batch_img2, labels in val_loader:\n",
    "            # Set variables for training\n",
    "            batch_img1 = batch_img1.float().to(dev)\n",
    "            batch_img2 = batch_img2.float().to(dev)\n",
    "            labels = labels.long().to(dev)\n",
    "\n",
    "            # Get predictions and calculate loss\n",
    "            cd_preds = model(batch_img1, batch_img2)\n",
    "\n",
    "            cd_loss = criterion(cd_preds, labels)\n",
    "\n",
    "            cd_preds = cd_preds[-1]\n",
    "            _, cd_preds = torch.max(cd_preds, 1)\n",
    "\n",
    "            # Calculate and log other batch metrics\n",
    "            cd_corrects = (100 *\n",
    "                           (cd_preds.squeeze().byte() == labels.squeeze().byte()).sum() /\n",
    "                           (labels.size()[0] * (opt.patch_size**2)))\n",
    "\n",
    "            cd_val_report = prfs(labels.data.cpu().numpy().flatten(),\n",
    "                                 cd_preds.data.cpu().numpy().flatten(),\n",
    "                                 average='binary',\n",
    "                                 zero_division=0,\n",
    "                                 pos_label=1)\n",
    "\n",
    "            val_metrics = set_metrics(val_metrics,\n",
    "                                      cd_loss,\n",
    "                                      cd_corrects,\n",
    "                                      cd_val_report,\n",
    "                                      scheduler.get_last_lr())\n",
    "\n",
    "            # log the batch mean metrics\n",
    "            mean_val_metrics = get_mean_metrics(val_metrics)\n",
    "\n",
    "            for k, v in mean_train_metrics.items():\n",
    "                writer.add_scalars(str(k), {'val': v}, total_step)\n",
    "\n",
    "            # clear batch variables from memory\n",
    "            del batch_img1, batch_img2, labels\n",
    "\n",
    "        logging.info(\"EPOCH {} VALIDATION METRICS\".format(epoch)+str(mean_val_metrics))\n",
    "\n",
    "        \"\"\"\n",
    "        Store the weights of good epochs based on validation results\n",
    "        \"\"\"\n",
    "        if ((mean_val_metrics['cd_precisions'] > best_metrics['cd_precisions'])\n",
    "                or\n",
    "                (mean_val_metrics['cd_recalls'] > best_metrics['cd_recalls'])\n",
    "                or\n",
    "                (mean_val_metrics['cd_f1scores'] > best_metrics['cd_f1scores'])):\n",
    "\n",
    "            # Insert training and epoch information to metadata dictionary\n",
    "            logging.info('updata the model')\n",
    "            metadata['validation_metrics'] = mean_val_metrics\n",
    "\n",
    "            # Save model and log\n",
    "            if not os.path.exists('./tmp'):\n",
    "                os.mkdir('./tmp')\n",
    "            with open('./tmp/metadata_epoch_' + str(epoch) + '.json', 'w') as fout:\n",
    "                json.dump(metadata, fout)\n",
    "\n",
    "            torch.save(model, './tmp/checkpoint_epoch_'+str(epoch)+'.pt')\n",
    "\n",
    "            # comet.log_asset(upload_metadata_file_path)\n",
    "            best_metrics = mean_val_metrics\n",
    "\n",
    "\n",
    "        print('An epoch finished.')\n",
    "writer.close()  # close tensor board\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "from utils.parser import get_parser_with_args\n",
    "from utils.helpers import get_test_loaders\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# The Evaluation Methods in our paper are slightly different from this file.\n",
    "# In our paper, we use the evaluation methods in train.py. specifically, batch size is considered.\n",
    "# And the evaluation methods in this file usually produce higher numerical indicators.\n",
    "\n",
    "parser, metadata = get_parser_with_args()\n",
    "opt = parser.parse_args()\n",
    "\n",
    "dev = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "test_loader = get_test_loaders(opt)\n",
    "\n",
    "path = 'weights/snunet-32.pt'   # the path of the model\n",
    "model = torch.load(path)\n",
    "\n",
    "c_matrix = {'tn': 0, 'fp': 0, 'fn': 0, 'tp': 0}\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    tbar = tqdm(test_loader)\n",
    "    for batch_img1, batch_img2, labels in tbar:\n",
    "\n",
    "        batch_img1 = batch_img1.float().to(dev)\n",
    "        batch_img2 = batch_img2.float().to(dev)\n",
    "        labels = labels.long().to(dev)\n",
    "\n",
    "        cd_preds = model(batch_img1, batch_img2)\n",
    "        cd_preds = cd_preds[-1]\n",
    "        _, cd_preds = torch.max(cd_preds, 1)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(labels.data.cpu().numpy().flatten(),\n",
    "                        cd_preds.data.cpu().numpy().flatten()).ravel()\n",
    "\n",
    "        c_matrix['tn'] += tn\n",
    "        c_matrix['fp'] += fp\n",
    "        c_matrix['fn'] += fn\n",
    "        c_matrix['tp'] += tp\n",
    "\n",
    "tn, fp, fn, tp = c_matrix['tn'], c_matrix['fp'], c_matrix['fn'], c_matrix['tp']\n",
    "P = tp / (tp + fp)\n",
    "R = tp / (tp + fn)\n",
    "F1 = 2 * P * R / (R + P)\n",
    "\n",
    "print('Precision: {}\\nRecall: {}\\nF1-Score: {}'.format(P, R, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This file is used to save the output image\n",
    "'''\n",
    "\n",
    "import torch.utils.data\n",
    "from utils.parser import get_parser_with_args\n",
    "from utils.helpers import get_test_loaders, initialize_metrics\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "if not os.path.exists('./output_img'):\n",
    "    os.mkdir('./output_img')\n",
    "\n",
    "parser, metadata = get_parser_with_args()\n",
    "opt = parser.parse_args()\n",
    "\n",
    "dev = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "test_loader = get_test_loaders(opt, batch_size=1)\n",
    "\n",
    "path = 'weights/snunet-32.pt'   # the path of the model\n",
    "model = torch.load(path)\n",
    "\n",
    "model.eval()\n",
    "index_img = 0\n",
    "test_metrics = initialize_metrics()\n",
    "with torch.no_grad():\n",
    "    tbar = tqdm(test_loader)\n",
    "    for batch_img1, batch_img2, labels in tbar:\n",
    "\n",
    "        batch_img1 = batch_img1.float().to(dev)\n",
    "        batch_img2 = batch_img2.float().to(dev)\n",
    "        labels = labels.long().to(dev)\n",
    "\n",
    "        cd_preds = model(batch_img1, batch_img2)\n",
    "\n",
    "        cd_preds = cd_preds[-1]\n",
    "        _, cd_preds = torch.max(cd_preds, 1)\n",
    "        cd_preds = cd_preds.data.cpu().numpy()\n",
    "        cd_preds = cd_preds.squeeze() * 255\n",
    "\n",
    "        file_path = './output_img/' + str(index_img).zfill(5)\n",
    "        cv2.imwrite(file_path + '.png', cd_preds)\n",
    "\n",
    "        index_img += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
